{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMV9Nvry7eNd61HAxKmVrs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsu1214/class2022spring/blob/main/npl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrFHP79bXd0v"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk : natural language  \n",
        "artificial language와 반대 개념 (컴퓨터 언어와 반대인 사람의 언어를 말한다) text라고 생각하자  \n",
        "Tokenization: 긴 텍스트를 단어 수준의 단위로 끊는 것을 의미한다.  \n",
        "Normalization: 깔끔하게 하는 것, 동사에 접두사 붙고 명사에 접미사 붙는 것과 같ㅌ은 것을 정리하는 것   \n",
        "Stopword: 너무 많이 쓰는 조동사 대명사 관사 등에는 관심이 없다(the, a, is) 너무 자주 나오는 단어들은 분석에 쓸모 없기 때문에 불용어 리스트라고 한다.  \n",
        "Collocation: 연어, take care나 Michle Smith와 같은 따라 나오는 단어   \n",
        "Concordance: 단어가 나올 때 용례를 찾아주는 것  \n",
        "Frequency: 어떤 단어가 많이 사용되었는지 (pitcher는 야구에서 많이 사용)   \n",
        "Dictionary: 사전  \n",
        "pos tag: 품사의 정의와 설명, 예 등을 정리해놓은 것   \n",
        "Wordcloud: 단어의 빈도별로 글자의 크기를 달리 해서 정리한 것  \n",
        "Regular expression: 규칙을 표현하는 것, 어떤 텍스트에 서치나 조작을 하고 싶을 때 서치를 해서 찾아내는 것, 규칙들을 이용해 서치한다."
      ],
      "metadata": {
        "id": "-WXL96rCXzk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "55DGxICVaN-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy a file from github\n",
        "import os\n",
        "url = \"https://raw.githubusercontent.com/hsnam95/class2022Spring/main/crime_punishment.txt\"\n",
        "os.system(\"curl \" + url + \" > crime_punishment.txt\")\n",
        "\n",
        "# read a text file in the server\n",
        "file = open(\"crime_punishment.txt\")\n",
        "text = file.read().replace(\"\\n\", \" \")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "nRFfuUxQbc-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. \\\n",
        "The ones who see things differently — they’re not fond of rules. \\\n",
        "You can quote them, disagree with them, glorify or vilify them, \\\n",
        "but the only thing you can’t do is ignore them because they change things. \\\n",
        "They push the human race forward, and while some may see them as the crazy ones, we see genius, \\\n",
        "because the ones who are crazy enough to think that they can change the world, are the ones who do.'"
      ],
      "metadata": {
        "id": "XtssKmzRb60x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write out a text file\n",
        "file = open(\"tmp.txt\", \"w\")\n",
        "file.write(text)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "ewePVIO-cAMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tmt.txt하면 text를 파일화할 수 있다."
      ],
      "metadata": {
        "id": "gcXEBVAucD0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()"
      ],
      "metadata": {
        "id": "ERUnsA1gcTkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "띄어쓰기를 기준으로 쪼개준다 이때 쉼표도 전 단어에 포함해야 한다."
      ],
      "metadata": {
        "id": "kCoN_i5_cVYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(text.split())"
      ],
      "metadata": {
        "id": "zYtfQhvycl9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "띄어쓰기를 기준으로 list에 있는 토큰들을 다 결합해라"
      ],
      "metadata": {
        "id": "8SXpsw3gcosh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "words = word_tokenize(text)"
      ],
      "metadata": {
        "id": "wdJLRqGkdBvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에 실행을 해주면 쉼표나 마침표 같은 펑츄에이션도 쪼개주게 된다."
      ],
      "metadata": {
        "id": "PdJOOGrJdGvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "retokenize = RegexpTokenizer(\"[\\w]+\")\n",
        "words = retokenize.tokenize(text)"
      ],
      "metadata": {
        "id": "BtNLOXK9dQLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이는 우리가 마침표나 쉼표 같은 것들은 필요하지 않기 때문에 없애주는 역할을 한다."
      ],
      "metadata": {
        "id": "xLK8pr0kdTtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization  \n",
        "Stemming 어간 추출 대충의 패턴 규칙으로 어미를 잘라내는 것 (사전에 없는 어간 나올 수 있음)\n",
        "Lemmatization 표제어(기본 사전형) 추출"
      ],
      "metadata": {
        "id": "x7vOW8Z0dbQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "[stemmer.stem(w) for w in words]"
      ],
      "metadata": {
        "id": "wzZ3pvJUd508"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "[stemmer.stem(w) for w in words]"
      ],
      "metadata": {
        "id": "hF5W06Dbd7kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "둘 다 어미를 분리해주기는 하지만 매우 부정확함"
      ],
      "metadata": {
        "id": "hTwJCcxBd8fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "[lemmatizer.lemmatize(w) for w in words]"
      ],
      "metadata": {
        "id": "6DqITik9eFk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이상하게 잘라놓은 것 들을 다시 원상태로 돌려줌"
      ],
      "metadata": {
        "id": "H-Sck7fpeHVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopword"
      ],
      "metadata": {
        "id": "qkwhqzVOeLE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords  \n",
        "nltk.download('stopwords')\n",
        "print(words)\n",
        "words = [w for w in words if not w in stopwords.words('english')]\n",
        "print(words)"
      ],
      "metadata": {
        "id": "ZfTpJBrB2hXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "둘이 비교해보면 s나 to등이 빠져있는 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "1mVjwE4I2lYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collocation, Concordance  \n",
        "gutenberg는 기간이 지나 저작권이 없는 글들을 모아둔 것, 성경도 포함  \n",
        "text는 str로 들어가 있고 words는 retokenize했기 때문에 list로 들어가 있다."
      ],
      "metadata": {
        "id": "SsCAg-Cz3J96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n",
        "text = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
        "words = retokenize.tokenize(text)"
      ],
      "metadata": {
        "id": "A6a_ZxT13Y3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "collocation은 연속해서 나오는 두개의 단어   \n",
        "원래 뒤에 num이랑 window_size해줘야 하는데 안 하면 고정값으로 나온다.   \n",
        "num은 20개 찾으라는 것이고 window_size는 연속해서 나오는 단어의 갯수를 말한다. 3은 세개의 단어가 연속으로 나오는것을 의미"
      ],
      "metadata": {
        "id": "Tb24u4vw4S2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).collocations()  # default: (num=20, window_size=2)"
      ],
      "metadata": {
        "id": "iCayoELJ4o1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "concordance는 용례이다.\n",
        "Emma라는 단어가 등장하는 앞 뒤 context를 10개 찾아라  \n",
        "79는 그 단어가 들어간 문장을 찾는데 앞뒤로 79글자만 뽑아서 보여달라는 것을 의미"
      ],
      "metadata": {
        "id": "2hfUCTBb45JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).concordance('Emma', 79, 10)"
      ],
      "metadata": {
        "id": "C0I-bfZ04rn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dispersion plot   \n",
        "words에 들어가 있는 160000개의 단어들 중에서 이 단어가 나오는 곳은 어디인지 표로 보여주는 것  누가 주인공이고 조연인지 알 수 있다."
      ],
      "metadata": {
        "id": "QXz6wDIf52qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).dispersion_plot([\"Emma\", \"Knightley\", \"Frank\", \"Jane\", \"Harriet\", \"Robert\"])"
      ],
      "metadata": {
        "id": "yR1KA5HN6ZoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "similar는 분포 상으로 유사한 것이 무엇인가를 말한다.  \n",
        "Emma와 비슷하게 나온 것이 무엇인지 찾아라.  \n",
        "이렇게 하면 she나 it등과 같은 자주 나오는 단어들이 나온다."
      ],
      "metadata": {
        "id": "cyizyfg46h5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distributional similarity: \n",
        "# find other words which appear in the same contexts as the specified word; \n",
        "# list most similar words first.\n",
        "nltk.Text(words).similar(\"Emma\")"
      ],
      "metadata": {
        "id": "qDb1fvFi61h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "common context  \n",
        "이렇게 하면 단어 두개사이에_가 끼인 상태로 등장하는데 이 _ 사이에 Emma나 She 가 등장할 확률이 높다는 것을 의미"
      ],
      "metadata": {
        "id": "eUnZ1pLe62hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find contexts where the specified words appear; list most frequent common contexts first.\n",
        "nltk.Text(words).common_contexts([\"Emma\", \"she\"])"
      ],
      "metadata": {
        "id": "o8CuVEGX7JkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency distribution, Frequency plot"
      ],
      "metadata": {
        "id": "xvgdJAPn7KSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(words).most_common(20)\n",
        "fd"
      ],
      "metadata": {
        "id": "ASDhMgDg7Pex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게 해주면 특정 단어가 몇번 등장하는지 알려준다.   \n",
        "위의 것이랑 똑같은 것을 그래프로도 표현이 가능하다."
      ],
      "metadata": {
        "id": "phKWVrLW7RyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).plot(20)"
      ],
      "metadata": {
        "id": "9IGskwmF7cwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dictionary  \n",
        "여기서 들어간 en은 english를 의미한다."
      ],
      "metadata": {
        "id": "8CwrSsQb7d1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')\n",
        "nltk.corpus.words.words('en')[-20:-1]"
      ],
      "metadata": {
        "id": "hp7isEkP7xdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pos tag는 part of speech라 해서 품사이다.  \n",
        "접속사, 관사, 형용사, 대명사, 소유격, 동사 등이 있다."
      ],
      "metadata": {
        "id": "eLK8EW8yeTW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"I am Jhon from America and would like to go to Starbuck\"\n",
        "words = nltk.word_tokenize(sent)"
      ],
      "metadata": {
        "id": "jNi6SzGLemjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 tokenize를 해줘야 한다."
      ],
      "metadata": {
        "id": "CAHD4VE_eoVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos = nltk.pos_tag(words)"
      ],
      "metadata": {
        "id": "14A4ESdmesd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos"
      ],
      "metadata": {
        "id": "89wRGDPQeubm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "NE = nltk.ne_chunk(pos)\n",
        "# common Entity types: ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, and GPE (geo-political entity)"
      ],
      "metadata": {
        "id": "1tGTHRJqe3nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NE"
      ],
      "metadata": {
        "id": "7rCeGz_jfEBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 오류가 있지만 교수님도 모르신다...."
      ],
      "metadata": {
        "id": "LyO48kILfFXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Worldcloud\n",
        "freauency랑 관련 있고 글자 폰트를 다르게 해준다."
      ],
      "metadata": {
        "id": "T59UYGDLfK1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = nltk.corpus.gutenberg.raw('bible-kjv.txt')\n",
        "\n",
        "wc = WordCloud().generate(text) \n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "0IcrKlrUfZJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "원래 stopword는 nltk에서 다 정해져 있지만 우리가 추가할 수도 있다.  \n",
        "add unto 해서 unto를 추가할 수 있다."
      ],
      "metadata": {
        "id": "0lDpCijzfibj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(STOPWORDS) \n",
        "stopwords.add('unto')\n",
        "wc = WordCloud(stopwords = stopwords).generate(text) \n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "7GY6IPKOf0N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular expression   \n",
        "re.search는 왼쪽에 있는 str 사이에서 오른쪽을 찾으라는 이야기. 두개가 있더라도 제일 처음 발견되는 것만 발견한다."
      ],
      "metadata": {
        "id": "UeOeWASbtT7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.search('a', 'abcdefa')"
      ],
      "metadata": {
        "id": "Bj9jAWeZt4gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "re.findall 은 전부 다 찾아주는 것이다. 다 찾아서 list로 만들어준다."
      ],
      "metadata": {
        "id": "vUYHL1wLt_us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('a', 'abcdefa')"
      ],
      "metadata": {
        "id": "dAoiyAQmuTAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "re.sub는 replace한다는 것이다. a를 b로 바꾸어라"
      ],
      "metadata": {
        "id": "4TZpoFXtuTnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub('a', 'b', 'abcdefa')"
      ],
      "metadata": {
        "id": "C73ZLRdjue-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engdict = nltk.corpus.words.words('en')"
      ],
      "metadata": {
        "id": "kCldy6NKwtu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "사전은 list형태로 존재하고 있다. 하지만 search나 findall은 string일 때 작동한다. 따라서 engdict[10]이런식으로 가져와서 string 상태로 사용할 것이다."
      ],
      "metadata": {
        "id": "ZS_lO-c7w2wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = [w for w in engdict if re.search('ed$', w)]\n",
        "# result = [w for w in engdict if re.search('^..j..t..$', w)]\n",
        "# result = [w for w in engdict if re.search('^[ghi][mno][jlk][def]$', w)]\n",
        "# result = [w for w in engdict if re.search('^[ah]+$', w)][:10]\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "Pw0vDSkixpMz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}